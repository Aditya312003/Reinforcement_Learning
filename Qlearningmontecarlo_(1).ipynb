{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya312003/Reinforcement_Learning/blob/main/Qlearningmontecarlo_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv4te9SDovFo",
        "outputId": "2138479a-5315-4385-fb1f-1172549b6e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0:\n",
            "Q-table:\n",
            "[[-0.1  0.   0.   0.   0.   0. ]\n",
            " [10.   0.   0.   0.   0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.   0. ]\n",
            " [-0.1  0.   0.   0.   0.   0. ]\n",
            " [-0.1  0.   0.   0.   0.   0. ]\n",
            " [ 0.   0.   0.   0.   0.   0. ]]\n",
            "---------------------------------\n",
            "Episode 50:\n",
            "Q-table:\n",
            "[[25.74743402  3.35010183  2.51679357  5.26688076  2.25021668  1.7808501 ]\n",
            " [69.14808162 10.         27.1         0.         15.94065814 23.16969379]\n",
            " [ 0.          0.          0.          0.          0.          0.        ]\n",
            " [ 1.94157118 14.76405423 42.5319488   0.61003403 10.95076365  0.76272998]\n",
            " [ 8.37665136 13.2976493   4.9171059  12.59791959 29.1387671   8.69829318]\n",
            " [ 0.          0.          0.          0.          0.          0.        ]]\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearning:\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "    def epsilon_greedy_policy(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.n_actions)\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def learn(self, num_episodes, display_interval=50):\n",
        "        for episode in range(num_episodes):\n",
        "            if episode % display_interval == 0:\n",
        "                print(f\"Episode {episode}:\")\n",
        "\n",
        "            state = 0  # Start from the initial state\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "\n",
        "            while not done:\n",
        "                action = self.epsilon_greedy_policy(state)\n",
        "\n",
        "                # Define transitions based on your custom environment\n",
        "                if state == 1:\n",
        "                    next_state = np.random.choice([3, 5])  # Randomly choose between 3 and 5\n",
        "                elif state == 3:\n",
        "                    next_state = np.random.choice([1, 4])  # Randomly choose between 1 and 4\n",
        "                elif state == 5:\n",
        "                    next_state = np.random.choice([1, 4, 5])  # Randomly choose between 1, 4, and 5\n",
        "                elif state == 2:\n",
        "                    next_state = 3\n",
        "                elif state == 4:\n",
        "                    next_state = np.random.choice([0, 3])  # Randomly choose between 0 and 3\n",
        "                elif state == 0:\n",
        "                    next_state = 4\n",
        "                else:\n",
        "                    next_state = state\n",
        "\n",
        "                if next_state == 5:\n",
        "                    reward = 100\n",
        "                else:\n",
        "                    reward = -1\n",
        "\n",
        "                # Update the Q-table\n",
        "                self.Q[state, action] += self.alpha * (reward + self.gamma * np.max(self.Q[next_state]) - self.Q[state, action])\n",
        "\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "                if next_state == 5:\n",
        "                    done = True\n",
        "\n",
        "            if episode % display_interval == 0:\n",
        "                print(\"Q-table:\")\n",
        "                print(self.Q)\n",
        "                print(\"---------------------------------\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    n_states = 6  # Number of states (0 to 5)\n",
        "    n_actions = 6  # Number of actions (0 to 5)\n",
        "    agent = QLearning(n_states, n_actions, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
        "    agent.learn(100, display_interval=50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def monte_carlo_pi(num_samples):\n",
        "    inside_circle = 0\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        x = random.uniform(0, 1)\n",
        "        y = random.uniform(0, 1)\n",
        "\n",
        "        # Check if the point is inside the unit circle\n",
        "        if x**2 + y**2 <= 1:\n",
        "            inside_circle += 1\n",
        "\n",
        "    # Calculate the estimated value of π\n",
        "    pi_estimate = (inside_circle / num_samples) * 4\n",
        "    return pi_estimate\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_samples = 1000000  # Number of random samples\n",
        "    estimated_pi = monte_carlo_pi(num_samples)\n",
        "    print(f\"Estimated π using Monte Carlo: {estimated_pi}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epOgerU30xIF",
        "outputId": "324e5c1a-6351-4ffb-9355-ddf290bacdec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated π using Monte Carlo: 3.141904\n"
          ]
        }
      ]
    }
  ]
}